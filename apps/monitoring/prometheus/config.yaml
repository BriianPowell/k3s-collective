#
# Reference:
# https://www.opsramp.com/guides/prometheus-monitoring/prometheus-alerting/
# https://prometheus.io/docs/introduction/overview/
# https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus-kubernetes.yml
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  namespace: monitoring
  labels:
    app: prometheus
data:
  prometheus.rules: |-
    groups:
      - name: Deployment groups
        rules:
          - alert: Deployment at 0 Replicas
            expr: sum(kube_deployment_status_replicas{pod_template_hash=""}) by (deployment,namespace)  < 1
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Deployment {{$labels.deployment}} is currently having no pods running
      - name: Pods
        rules:
          - alert: High Pod Memory
            expr: sum(container_memory_working_set_bytes{image!="",container!="POD", namespace!~"kube-system|valheim|minecraft"}) by (namespace,container,pod) > 1200000000
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Pod {{$labels.pod}} is currently utilizing high memory (> 1.2GB)
          - alert: High Game Pod Memory
            expr: sum(container_memory_working_set_bytes{image!="",container=~"valheim-server|minecraft-server-minecraft", namespace=~"valheim|minecraft"}) by (namespace,container,pod) > 7000000000
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Pod {{$labels.pod}} is currently utilizing high memory (> 5GB)
          - alert: Container Restarted
            expr: sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[1m])) by (pod,namespace,container) > 0
            for: 0m
            labels:
              severity: warn
            annotations:
              summary: Container {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} was restarted
          - alert: Too Many Container Restarts
            expr: sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[15m])) by (pod,namespace,container) > 5
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} was restarted
      - name: Node
        rules:
          # https://stackoverflow.com/questions/69281327/why-container-memory-usage-is-doubled-in-cadvisor-metrics/69282328#69282328
          # https://stackoverflow.com/questions/54866777/count-k8s-cluster-cpu-memory-usage-with-prometheus
          - alert: High Node Memory Usage
            expr: (sum(container_memory_usage_bytes{container!=""}) by (kubernetes_io_hostname) / sum (machine_memory_bytes{}) by (kubernetes_io_hostname)) * 100 > 80
            for: 5m
            labels:
              severity: warn
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% memory used
          - alert: High Node CPU Usage
            expr: (sum(rate(container_cpu_usage_seconds_total{id="/",container!="POD"}[1m])) by (kubernetes_io_hostname) / sum(machine_cpu_cores) by (kubernetes_io_hostname)  * 100) > 80
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% allocatable cpu used
          - alert: High Node Disk Usage
            expr: (sum(container_fs_usage_bytes{id="/",container!="POD"}) by (kubernetes_io_hostname) / sum(container_fs_limit_bytes{container!="POD",id="/"}) by (kubernetes_io_hostname)) * 100 > 85
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 85% disk used
  prometheus.yml: |-
    global:
      scrape_interval: 15s
      evaluation_interval: 20s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
        - scheme: http
          static_configs:
            - targets:
              - "alertmanager.monitoring.svc:9093"

    scrape_configs:
      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Scrape config for nodes (kubelet).
      #
      # Rather than connecting directly to the node, the scrape is proxied though the
      # Kubernetes apiserver.  This means it will work if Prometheus is running out of
      # cluster, or can't connect to nodes for some other reason (e.g. because of
      # firewalling).
      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

      # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
      # (those whose names begin with 'container_') have been removed from the
      # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
      # retrieve those metrics.
      #
      # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
      # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
      # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
      # the --cadvisor-port=0 Kubelet flag).
      #
      # This job is not necessary and should be removed in Kubernetes 1.6 and
      # earlier versions, or it will cause the metrics to be scraped twice.
      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name

      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
      # pod's declared ports (default is a port-free target if none are declared).
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.kube-system.svc.cluster.local:8080']

      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_endpoints_name]
            regex: 'node-exporter'
            action: keep

      - job_name: 'hass'
        scrape_interval: 60s
        metrics_path: /api/prometheus
        authorization:
          credentials: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiIxMjRmNTFlZWI0MWU0MjU5YThhZTYwYzQ1MGY2NDc2MyIsImlhdCI6MTY3NTEyNDIwMywiZXhwIjoxOTkwNDg0MjAzfQ.eQM5cK78E6KJCIrMyFlElaGMqMaaWIuhECGeIc3TuR0
        scheme: http
        static_configs:
          - targets: ['homeassistant.homeassistant.svc.cluster.local:8123']

      # - job_name: 'crowdsec' # Security related metrics
      #   # https://doc.crowdsec.net/docs/observability/prometheus
      #   static_configs:
      #     - targets: ['crowdsec-service.crowdsec:8080']
      #       # labels:
      #       #   host: sheol
      #   metric_relabel_configs:
      #     # Only keep data that is used in graphs to reduce size
      #     - source_labels: [__name__]
      #       regex: cs_.+
      #       action: keep
